{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Getting acquainted with pre-trained deep neural networks and PyTorch\n",
    "\n",
    "### General instructions\n",
    "\n",
    "Each week you will be given an assignment related to the associated module. You have roughly one week to complete and submit each of them. There are 3 weekly group sessions available to help you complete the assignments, you are invited to attend one of them each week (please choose one group session and stick to it unless exceptional case). Attendance is not mandatory but recommended. However, **assignments are graded** each week and not submitting them or submitting them after the deadline will give you no points.\n",
    "\n",
    "This jupyter notebook is the first weekly assignment of this course. We strongly recommend you to use anaconda to manage your python packages as well as for installing jupyter if it is not already done. For this course we will use PyTorch so you probably need to install some of the packages imported in the first cell of this notebook. \n",
    "\n",
    "If you are new to jupyter notebook please follow this [link](https://realpython.com/jupyter-notebook-introduction/) or any beginner's tutorial that would suit you well.\n",
    "\n",
    "In this notebook you will be guided since it is the first assignment. At the beginning of this notebook, most cells just need to be run and you just have to read (and click on the links). Later, you will have to complete the code (when there are \"TODOS\" and \"...\"), answer the questions (you can either create a new cell below the questions or answer directly in the same cell as the questions) and submit an **archive** to MittUiB containing\n",
    "- your completed notebook\n",
    "- the file '``list_labels.txt``'\n",
    "- your images (in the '``imgs``' folder)\n",
    "\n",
    "Please include **your name** in your submitted file so that graders can easily distinguish between all students files\n",
    "\n",
    "Submit your archive by **Sunday 31st, 23:59.**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This week we will get a quick idea of what a deep neural network looks like and what it is capable of when it comes to image classification tasks. To do so we will play with a pre-trained neural network (ResNet101). In addition we will make our first steps in PyTorch.\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Deep neural network in PyTorch\n",
    "\n",
    "  1.1 Pre-trained deep neural network models available in PyTorch  \n",
    "  1.2. ResNet architecture and ResNet101 model  \n",
    "  1.3 Neural network implementation in PyTorch\n",
    "  1.4 Define a preprocess pipeline using PyTorch's transforms\n",
    "  \n",
    "2. Making predictions with a pre-trained network\n",
    "\n",
    "  2.1 Load and preprocess an image  \n",
    "  2.2 Extract labels with which ResNet was trained  \n",
    "  2.3 Interpreting the output\n",
    "  2.4 Top-1 and Top-5 errors\n",
    "\n",
    "  \n",
    "3. Playing with the ResNet model\n",
    "\n",
    "4. Good to know"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from os import listdir"
   ]
  },
  {
   "source": [
    "## 1. Deep neural network in PyTorch\n",
    "\n",
    "### 1.1 Pre-trained deep neural network models available in PyTorch\n",
    "\n",
    "As written in the documentation:\n",
    "\n",
    "> The [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html#torchvision-models) subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification. \\[...\\] It provides pre-trained models,"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['AlexNet', 'DenseNet', 'GoogLeNet', 'GoogLeNetOutputs', 'Inception3', 'InceptionOutputs', 'MNASNet', 'MobileNetV2', 'ResNet', 'ShuffleNetV2', 'SqueezeNet', 'VGG', '_GoogLeNetOutputs', '_InceptionOutputs', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_utils', 'alexnet', 'densenet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'detection', 'googlenet', 'inception', 'inception_v3', 'mnasnet', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet', 'mobilenet_v2', 'quantization', 'resnet', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'segmentation', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'shufflenetv2', 'squeezenet', 'squeezenet1_0', 'squeezenet1_1', 'utils', 'vgg', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'video', 'wide_resnet101_2', 'wide_resnet50_2']\n72\n"
     ]
    }
   ],
   "source": [
    "print(dir(models))\n",
    "print(len(dir(models)))"
   ]
  },
  {
   "source": [
    "### 1.2 ResNet architecture and ResNet101 model\n",
    "\n",
    "[ResNet](https://arxiv.org/abs/1512.03385) is a deep residual neural network that aims at classifying images. We can get an idea of its architecture by simply using ``print``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (6): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (7): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (8): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (9): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (10): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (11): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (12): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (13): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (14): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (15): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (16): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (17): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (18): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (19): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (20): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (21): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (22): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet101(pretrained=True)   # 101 means that we choose the ResNet architecture with 101 layers\n",
    "print(resnet)        # Instance of a ResNet model"
   ]
  },
  {
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "1. If we want to use 256x256 RGB images as input, what should be the dimension of the input layer of the neural network? \n",
    "2. If we have 1000 different labels (e.g cat, dog, mouse, goose, etc) what should be the dimension of the output layer of the neural network\n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "1. The input layer should be 3 input image channels (one for each color) with 256*256 pixels per channel, so in total a 256*256*3 pixel layer\n",
    "2. The output layer should be a layer with 1000 output-neurons, so having the dimension 1x1000, one output-neuron for each label."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.3 Neural network implementation in PyTorch\n",
    "\n",
    "In PyTorch all neural networks should be a class that is itself a subclass of the [PyTorch's torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module) class\n",
    "\n",
    "In the next 2 cells we'll show that [ResNet](https://pytorch.org/docs/stable/torchvision/models.html#id10) class is indeed a subclass of ``torch.nn.Module``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torchvision.models.resnet.ResNet'>\n"
     ]
    }
   ],
   "source": [
    "print(type(resnet))  # Find the python class of this instance (models.resnet.ResNet)"
   ]
  },
  {
   "source": [
    "Now that we know that our model is an instance of the class ``torchvision.models.resnet.ResNet`` we can check if this is a subclass of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "resnet_class = models.resnet.ResNet \n",
    "print(issubclass(resnet_class, torch.nn.Module))  # Show that the ResNet class is indeed a subclass of 'torch.nn.Module'"
   ]
  },
  {
   "source": [
    "### 1.4 Define a preprocess pipeline using PyTorch's transforms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms) module can easily performs the most common image transformations such as [resize](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize), [normalize](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize), etc. \n",
    "\n",
    "In addition, this module allow us to quickly define pipelines of basic preprocessing functions using the [transforms.Compose](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose) method.\n",
    "\n",
    "Thus in the following cell we define the pre-processing transformations that will be used on our input images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = transforms.Compose([\n",
    "    transforms.Resize(256),     # Resize to a 256x256 image\n",
    "    transforms.CenterCrop(224), # Crop the center (usually where the interesting object is)\n",
    "    transforms.ToTensor(),      # PyTorch's counterpart of Numpy's arrays\n",
    "    transforms.Normalize(       # Normalize input the same way ResNet training inputs were normalized \n",
    "    mean=[0.485, 0.456, 0.406], ### Mean given to match what was presented to ResNet during training\n",
    "    std=[0.229, 0.224, 0.225]   ### Same here\n",
    ")])"
   ]
  },
  {
   "source": [
    "## 2. Making predictions with a pre-trained network\n",
    "\n",
    "### 2.1 Load and preprocess an image \n",
    "\n",
    "In PyTorch, the data are stored in [tensors](https://pytorch.org/docs/stable/tensors.html#torch.Tensor). This is the counterpart of Numpy's arrays and most of the methods that are available with numpy arrays are also available with tensors. (e.g \n",
    "[size](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size), \n",
    "[amax](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.amax), \n",
    "[argmax](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.argmax), \n",
    "[sort](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sort), \n",
    "[abs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.abs), \n",
    "[cos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cos), \n",
    "[sum](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum) etc.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imgs/Bobby.jpeg\")\n",
    "img_t = preprocessor(img)             # Preprocess our image using our preprocessor ('t' stands for 'tensor')\n",
    "batch_t = torch.unsqueeze(img_t, 0)   # Reshape so that it is a batch (of size 1) as required by the model "
   ]
  },
  {
   "source": [
    "### 2.2 Extract labels with which ResNet was trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the labels with which ResNet was trained and store them in the list 'labels'\n",
    "with open('list_labels.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "source": [
    "As stated in the PyTorch documentation: \n",
    "\n",
    "> \"Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use [model.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train) or [model.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval) (from the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)) as appropriate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "resnet.eval()  # Pytorch method to indicate that we are now using the model to make predictions and not to train it anymore \n",
    "print(\" \")"
   ]
  },
  {
   "source": [
    "Now we are ready to make some predictions on our images! \n",
    "Let's show the output of the resnet model given our image of Bobby the Golden Retriever\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "1. Print the dimension of the tensor ``out`` using the [Tensor.size()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size) method\n",
    "2. Does it match your previous answer about the output dimension? \n",
    "\n",
    "**ANSWER**\n",
    "\n",
    "2. The output dimension matches the idea of my previous answer. I didn't expect the tensor to explicitly have size 1 in the first dimension and it therefore being a 2D-tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-3.4803e+00, -1.6618e+00, -2.4515e+00, -3.2662e+00, -3.2466e+00,\n         -1.3611e+00, -2.0465e+00, -2.5112e+00, -1.3043e+00, -2.8900e+00,\n         -1.6862e+00, -1.3055e+00, -2.6129e+00, -2.9645e+00, -2.4300e+00,\n         -2.8143e+00, -3.3019e+00, -7.9404e-01, -6.5182e-01, -1.2308e+00,\n         -3.0193e+00, -3.9457e+00, -2.2675e+00, -1.0811e+00, -1.0232e+00,\n         -1.0442e+00, -3.0918e+00, -2.4613e+00, -2.1964e+00, -3.2354e+00,\n         -3.3013e+00, -1.8553e+00, -2.0921e+00, -2.1327e+00, -1.9102e+00,\n         -3.2403e+00, -1.1396e+00, -1.0925e+00, -1.2186e+00, -9.3332e-01,\n         -4.5093e-01, -1.5489e+00,  1.4161e+00,  1.0871e-01, -1.8442e+00,\n         -1.4806e+00,  9.6227e-01, -9.9456e-01, -3.0060e+00, -2.7384e+00,\n         -2.5798e+00, -2.0666e+00, -1.8022e+00, -1.9328e+00, -1.7726e+00,\n         -1.3041e+00, -4.5848e-01, -2.0537e+00, -3.2804e+00, -5.0451e-01,\n         -3.8174e-01, -1.1147e+00, -7.3998e-01, -1.4299e+00, -1.4883e+00,\n         -2.1073e+00, -1.7373e+00, -4.0412e-01, -1.9374e+00, -1.4862e+00,\n         -1.2102e+00, -1.3223e+00, -1.0832e+00,  7.9208e-02, -4.1344e-01,\n         -2.7477e-01, -8.5398e-01,  6.0364e-01, -8.9196e-01,  1.4761e+00,\n         -2.6427e+00, -3.6478e+00, -2.7066e-01, -1.2360e-01, -2.2445e+00,\n         -2.3425e+00, -1.4430e+00,  2.5264e-01, -1.0588e+00, -2.8812e+00,\n         -2.5145e+00, -2.2579e+00,  4.1647e-01, -1.3463e+00, -1.6451e-02,\n         -2.8798e+00, -5.5658e-01, -1.3859e+00, -2.9352e+00, -1.8880e+00,\n         -4.2244e+00, -2.9742e+00, -2.0298e+00, -2.3869e+00, -2.7324e+00,\n         -3.9905e+00, -3.6113e+00, -5.4423e-01, -1.0291e+00, -1.8998e+00,\n         -3.5611e+00, -1.5031e+00,  1.0660e+00, -7.1587e-01, -7.2612e-01,\n         -2.2173e+00, -2.2616e+00, -5.9990e-01, -1.4349e+00, -2.5965e+00,\n         -3.9844e+00, -9.4164e-01, -5.3675e-01, -8.4138e-01, -1.1660e+00,\n         -7.3556e-01, -1.1300e+00, -2.1074e+00, -4.0037e+00, -3.7230e-01,\n         -2.7179e+00, -2.9849e+00, -1.9127e+00, -1.8412e+00, -1.3001e+00,\n         -2.2268e+00, -2.0247e+00, -3.1761e+00, -3.2964e+00, -2.7923e+00,\n         -4.3191e-01, -3.7750e+00, -2.4832e+00, -2.6228e+00, -2.7499e+00,\n         -2.6306e+00, -3.2714e+00, -4.3249e+00, -4.2451e+00, -3.6207e+00,\n         -1.1967e+00,  2.3839e+00,  1.8833e+00,  2.2390e+00,  4.9467e+00,\n          9.9434e-01,  2.9570e+00,  8.5852e-01,  2.2356e+00,  6.1872e+00,\n          4.2074e+00,  4.6280e+00,  7.5066e+00,  4.3456e+00,  4.8873e+00,\n          5.8086e+00,  4.0282e+00,  3.5778e+00,  9.5398e+00,  1.0959e+00,\n          3.3065e+00,  1.9473e+00, -4.7347e-01,  1.4388e+00,  1.8860e+00,\n          5.5149e+00,  5.6885e+00,  2.1434e+00,  2.5016e+00,  6.2614e-01,\n          1.9095e+00,  1.4927e+00,  3.4522e+00,  4.0987e-01,  4.2790e+00,\n          4.3379e+00,  1.2945e+00,  1.6308e+00,  1.1426e+00,  2.1246e+00,\n          8.6189e-01,  3.0266e+00,  3.5030e+00,  2.7914e+00,  1.8812e+00,\n          1.3916e-01,  2.0182e+00,  2.6938e+00,  1.0643e+00,  1.9063e+00,\n          3.5028e+00,  2.2950e+00,  2.5388e+00,  1.3140e+00,  3.5698e+00,\n          7.7051e+00,  4.3442e+00,  1.5674e+01,  1.2140e+01,  5.2050e+00,\n          1.9331e+00,  5.4996e+00,  6.1745e+00,  7.5155e+00,  5.8567e+00,\n          6.9794e+00,  5.6891e+00,  2.6934e+00,  5.3248e+00,  9.8436e+00,\n          6.4168e+00,  2.4431e+00,  5.6031e+00,  3.4884e+00,  2.0732e+00,\n          1.3375e+00,  2.5550e+00,  5.7791e+00,  7.5825e-01,  1.0360e+00,\n          4.8250e+00,  5.9932e+00,  3.9907e+00, -1.7508e+00,  3.6606e+00,\n          2.8820e+00,  2.8978e+00,  1.3059e+00,  4.2622e+00,  4.0880e+00,\n          3.4181e+00,  2.3945e+00,  3.1604e-01,  8.7091e-01,  5.0895e+00,\n         -7.0908e-01,  1.9885e+00,  2.8699e+00,  2.5281e+00,  1.9253e+00,\n          6.5843e-01,  3.4956e+00, -5.6701e-01,  1.9219e+00,  5.0423e-01,\n          2.3949e+00,  3.4628e+00,  5.1851e+00,  1.8182e+00,  3.9127e+00,\n          4.3620e+00,  3.3723e-01, -4.6588e-01,  5.6958e+00,  3.7192e+00,\n          2.4205e+00,  3.6402e+00,  3.3705e+00, -9.3733e-01, -2.0590e-01,\n          1.3019e-01,  1.1554e+00, -4.0951e-02,  4.5523e+00, -1.8349e+00,\n         -2.6543e+00, -1.6859e+00, -6.3751e-01, -1.5596e+00, -2.1529e+00,\n         -1.0245e+00,  1.5312e+00,  7.6857e-01, -1.8030e+00,  6.9034e-01,\n          9.1473e-01, -2.0907e+00, -2.1250e+00, -1.5808e+00, -4.7830e+00,\n         -1.0396e+00, -9.7836e-01, -2.0528e+00,  1.9793e+00, -6.0107e-01,\n         -2.4964e+00, -1.4914e+00, -3.2041e+00, -1.9067e+00, -5.9215e-01,\n         -1.0509e+00,  1.3131e+00, -1.5027e+00, -2.0352e+00,  1.3009e+00,\n          3.9806e-01, -3.5442e-01,  7.1537e-01, -3.0086e-01, -7.6253e-01,\n         -5.4504e-01,  1.0533e+00,  1.1973e-01,  7.1266e-02,  1.3234e+00,\n         -2.0051e+00, -1.7127e+00,  1.1415e+00, -4.3746e-01,  2.9573e-01,\n         -1.4572e+00, -2.6234e+00, -2.5400e+00, -2.4128e-01, -2.3629e+00,\n         -1.5560e+00, -2.5256e+00, -8.0395e-01,  1.5960e-01, -2.8029e+00,\n         -1.8937e+00, -9.4297e-01, -3.8987e-01, -4.6732e-01, -7.8798e-01,\n         -2.5103e+00, -1.8726e+00, -2.1138e+00, -5.5075e-01,  1.8876e-01,\n         -2.0678e+00, -1.7942e+00, -2.4776e+00, -3.8874e+00, -4.4214e+00,\n         -2.1606e+00, -1.9960e+00, -3.7195e+00, -1.8627e+00, -3.3882e+00,\n         -2.0034e+00, -2.2823e+00, -8.3603e-01, -5.1364e-01, -2.9197e+00,\n         -1.6728e+00, -2.5686e-01, -2.7734e+00, -1.7911e+00,  1.1284e-01,\n         -2.1215e+00, -1.5402e+00, -1.2457e+00, -9.6399e-01, -2.4953e+00,\n         -1.3973e+00, -3.8589e+00, -4.3189e+00, -1.5287e+00, -1.9420e+00,\n         -3.0008e+00, -2.9597e+00, -4.8460e+00, -2.4737e+00, -1.4287e+00,\n         -2.9093e+00, -1.2882e+00, -6.0873e-01, -2.8312e+00, -1.8754e+00,\n         -2.3758e+00, -3.4176e+00, -2.5520e+00, -3.8709e+00, -4.4702e+00,\n         -3.5587e+00, -9.4389e-01, -2.3503e+00, -2.0270e+00, -1.8470e+00,\n         -3.2897e+00, -3.4712e+00, -2.8471e+00, -1.9893e+00, -3.7441e+00,\n         -1.1865e+00, -2.8282e+00,  2.2838e-01, -1.3325e-01, -3.1261e-01,\n          1.4785e-01,  1.7180e+00,  1.8871e+00, -3.1302e+00, -3.7345e+00,\n         -2.6754e+00, -6.7742e-01, -8.4727e-01, -1.3179e+00,  4.7847e-01,\n         -2.2918e+00,  4.7733e+00,  1.5100e+00, -1.5956e+00,  3.3496e+00,\n          3.0611e+00,  1.5253e+00,  6.8673e-01,  1.2918e+00,  1.6387e+00,\n          1.0631e-01,  1.3420e+00,  5.2415e-02,  1.0270e+00, -4.6863e-01,\n         -1.3585e+00,  5.7504e-01,  2.8775e-01,  2.8255e+00,  2.1875e+00,\n          1.8301e+00,  1.3566e+00,  1.0992e+00,  2.3171e+00,  6.4046e+00,\n          1.8630e+00,  6.0024e-01, -1.4953e+00, -1.9144e+00, -2.6436e+00,\n          1.5186e+00, -4.8838e-01, -1.0530e-01,  1.9803e+00, -1.7358e+00,\n          3.7236e-01,  1.6658e+00,  7.8257e-01,  2.1721e+00, -1.4210e+00,\n         -2.4550e+00,  4.6637e-01,  3.3418e+00, -2.8537e-01,  1.1941e-01,\n          1.1450e+00, -1.3834e+00,  1.5737e+00, -2.1716e+00, -4.2427e-01,\n         -1.4805e+00, -2.1744e+00,  2.7962e+00,  2.4990e+00,  1.9237e-01,\n          4.7498e-01, -1.9682e+00, -1.6105e+00, -7.3869e-01, -1.1794e+00,\n         -2.9531e-01, -1.4142e+00,  2.2398e+00, -4.3380e-01, -8.6286e-01,\n          4.0300e-01, -1.4318e+00, -3.1364e-01,  3.4846e+00,  4.3202e-01,\n          4.5058e-01, -1.1090e+00,  2.2513e-01, -2.6651e+00, -2.8278e+00,\n         -6.5790e-01, -3.0889e-01,  8.2096e-01,  1.8005e-01, -4.2284e-01,\n         -5.8541e-01, -2.7820e-01,  1.6590e+00,  8.7698e-02, -4.6729e-01,\n          1.1241e+00,  2.2742e+00, -1.0448e+00,  9.4819e-01,  9.9525e-01,\n         -2.5969e+00, -5.5236e-01,  2.1583e+00, -9.2215e-01,  4.7107e-02,\n         -3.8016e-01,  1.5210e+00, -1.0433e+00,  1.9041e+00,  1.4741e+00,\n         -4.3896e+00, -1.6206e-01, -1.5698e-01, -7.3738e-01,  1.8179e+00,\n          3.3264e+00,  7.3696e-01, -7.6419e-01,  1.5898e+00,  1.9445e+00,\n          1.2725e+00, -1.5624e+00,  2.2197e+00,  9.9570e-01, -6.3256e-01,\n         -1.4160e+00,  1.6144e+00,  4.5531e-02,  9.0731e-01,  9.5069e-01,\n          5.3562e-01,  4.4124e-01,  1.0358e+00,  6.5592e-01,  3.3626e+00,\n         -1.0299e+00, -2.8939e+00, -7.0227e-01, -8.1103e-01,  7.0547e+00,\n         -3.3097e+00,  1.3230e+00,  1.6967e+00,  3.7732e+00, -1.1723e+00,\n          5.7985e-01, -1.8231e+00, -1.3483e+00,  4.1487e-01,  2.6429e+00,\n          1.4418e+00,  7.9635e-01,  4.8719e+00,  1.5457e+00, -3.5932e+00,\n         -2.2285e+00, -1.3850e+00, -8.9728e-01,  2.1657e+00,  2.0583e+00,\n         -8.9567e-01, -1.7835e+00, -1.4516e+00,  1.0497e+00, -7.6032e-01,\n         -1.4353e+00,  4.7010e-01,  8.7255e-01,  6.7030e-01, -1.1902e+00,\n         -1.4175e+00, -8.4839e-01,  1.1901e+00, -1.8283e+00,  2.4775e+00,\n          3.4005e-01, -1.7652e+00, -9.1973e-01,  2.9893e+00,  2.2373e+00,\n         -8.1442e-01, -1.9843e+00,  9.2510e-01, -2.1452e+00,  1.8890e-02,\n          2.5441e-01, -1.1333e-01, -6.2533e-01,  8.0225e-01,  4.0010e+00,\n         -1.1935e+00,  2.6455e+00, -1.7860e+00,  7.5865e-01,  5.1593e-01,\n          2.4370e-03, -7.6759e-01,  4.8149e-01,  1.3055e+00,  8.0364e-01,\n         -6.1874e-01,  4.6969e-02,  2.6322e-01, -2.1400e+00, -1.3908e+00,\n         -4.0182e-02, -4.2920e-01,  4.6767e-01,  1.3024e+00,  7.5817e-01,\n          9.9857e-02, -1.0072e-01, -8.5241e-01,  8.6250e-01,  6.9517e-01,\n          2.1217e+00,  7.1266e-01, -1.9782e-01,  2.3986e+00,  1.8734e+00,\n          1.0993e+00,  1.0336e+00,  1.4353e+00, -4.9214e-02, -1.3295e-01,\n         -1.7147e+00, -1.2590e+00, -1.3166e+00, -3.4476e+00,  5.9193e-01,\n          1.0995e+00,  1.0987e-02, -3.7005e-01, -4.5369e-01, -4.2330e-01,\n         -1.5137e+00,  2.7933e-01, -2.0776e-01,  3.2132e+00,  1.8063e+00,\n         -1.5186e+00,  2.8835e+00, -7.4290e-01,  3.2128e-02, -7.0118e-02,\n         -1.0103e+00,  1.1795e+00,  5.9283e-01,  1.2191e-01, -3.4571e+00,\n          1.3048e+00,  3.9847e-01, -1.2731e+00, -1.2927e+00, -1.6408e+00,\n          1.9229e+00,  4.1589e-02, -9.8907e-01,  6.7141e-01,  2.8807e+00,\n          1.6977e+00,  2.2305e-01, -8.1440e-01, -2.0507e+00,  1.7015e+00,\n         -2.0312e-01,  7.4630e-01,  1.5227e+00, -1.4377e+00, -1.1784e+00,\n          5.1375e-01, -6.4234e-01,  3.8709e-02,  2.6664e+00, -1.6256e+00,\n         -3.3457e+00,  2.1520e+00,  8.6618e-01,  1.3850e+00, -3.4029e-01,\n          1.8385e-01,  1.4680e+00, -1.0961e+00,  1.8217e+00, -1.2748e+00,\n         -2.1175e+00, -8.4857e-01, -5.3657e-01, -1.2562e+00,  1.1329e+00,\n         -1.4191e+00, -7.6893e-01, -3.4133e-01,  2.1594e+00, -2.1836e-01,\n         -1.8166e+00,  9.8037e-02,  1.7366e+00,  1.6465e-01,  7.7770e-01,\n          4.7226e+00, -7.3754e-01, -1.6683e+00, -8.1360e-01, -1.4618e+00,\n          3.4068e+00,  5.3348e-01, -3.1106e-01, -5.0764e-01,  3.0037e-01,\n          1.8626e+00, -1.1852e+00, -2.0411e+00, -9.6966e-02, -7.1424e-01,\n         -2.5433e+00, -3.4144e-02,  7.6702e-01, -1.7948e+00,  2.9510e-01,\n         -1.0903e+00,  1.5320e+00,  2.8823e+00,  5.1182e-01, -7.6857e-01,\n         -9.0145e-01, -1.7196e+00, -1.0044e+00,  9.1568e-01, -9.2979e-02,\n         -2.3068e+00,  2.2911e+00,  9.5719e-01,  1.9917e+00, -1.6980e+00,\n          2.6118e+00,  3.7953e+00,  7.1091e-01, -2.2800e-03, -1.0275e+00,\n          2.1824e+00,  1.4127e+00,  4.7933e-01, -1.3249e+00, -9.0533e-01,\n          5.8118e-01, -6.0400e-01,  5.1156e-01,  1.1511e+00,  9.5682e-01,\n          2.7826e+00, -3.0976e+00,  3.5563e+00, -1.6181e-01, -4.6196e-02,\n         -2.0769e+00, -1.4204e+00,  2.9824e+00, -4.8723e-01,  2.1408e-01,\n         -1.3643e-01,  2.2942e+00,  3.4084e-01,  9.9796e-01, -1.1452e+00,\n          3.3055e+00, -1.8049e+00,  3.2445e+00, -1.6493e-01,  1.3805e+00,\n          6.5878e-01,  4.6122e-01, -7.8641e-01,  3.8983e-01,  1.9974e+00,\n          4.0911e-01,  2.4162e+00, -1.9111e+00,  8.1045e-02,  2.2694e+00,\n         -1.6680e+00, -7.0304e-01,  1.4299e+00,  1.4234e-02,  7.9249e-01,\n          2.9637e+00, -9.4825e-01, -1.3366e+00,  2.6750e-01,  2.3589e+00,\n          1.8983e+00,  1.8345e+00,  8.5127e-01,  4.2841e+00,  4.8082e-01,\n         -1.4365e+00, -4.8286e-01,  3.0412e+00, -8.2024e-01,  3.3065e+00,\n         -6.5939e-01, -2.6282e+00, -3.1888e+00, -2.9725e+00,  1.2156e+00,\n          5.6016e+00,  3.0274e-01, -3.1681e+00,  2.5582e+00, -3.3199e-01,\n          1.4820e-01,  2.3601e+00, -1.4552e+00,  3.3269e+00, -3.3744e+00,\n         -6.4104e-01,  1.1680e+00, -2.6107e+00,  1.6885e+00, -1.5028e+00,\n         -2.6845e+00, -3.6659e+00, -1.7394e+00,  1.1231e+00,  2.0104e+00,\n         -1.4943e-01,  1.3057e+00,  1.2092e+00,  2.6647e+00, -1.7969e+00,\n         -1.8525e+00,  1.5488e+00, -2.0861e+00, -2.3154e+00,  9.9215e-01,\n         -3.7871e+00, -1.1176e+00,  9.0636e-01, -3.2947e-01, -3.4544e+00,\n          2.0940e+00,  5.4372e-01,  6.0876e-01, -1.3066e-01,  7.9443e-01,\n          7.9938e-01,  1.0587e+00, -1.8372e+00,  2.8466e-01, -1.1158e+00,\n          8.0786e-01,  1.0870e+00,  8.9547e+00, -8.9419e-01, -9.3960e-01,\n          1.0806e+00, -4.1462e-01, -1.7524e+00,  9.1855e-02,  1.8185e-01,\n         -1.3849e+00,  8.8831e-01, -4.1253e-01, -7.7844e-01, -3.1265e+00,\n         -3.8734e-01,  1.8115e-01, -2.2122e+00,  2.8848e+00,  4.5000e-01,\n          1.4854e+00, -3.4138e+00,  1.4939e+00, -2.5266e+00, -2.9228e+00,\n         -7.6507e-01,  2.8269e+00, -1.1918e+00, -6.2602e-01,  3.6187e+00,\n          1.1527e+00,  1.1860e+00,  3.4149e+00,  9.2982e-01, -1.1376e+00,\n          1.0391e+00,  1.8575e-01, -7.4427e-01, -2.9312e+00, -1.6815e-01,\n          1.5624e+00, -4.5063e-01,  1.5997e+00,  1.0128e+00, -1.3146e+00,\n         -1.8426e+00, -4.7445e-01,  5.8991e-01,  2.3850e+00,  5.2548e-01,\n         -1.3760e+00, -2.3240e+00, -7.6861e-01,  1.2772e+00,  2.9579e+00,\n         -2.7968e-01, -5.9378e-01, -2.4310e-02, -7.2352e-01, -5.9499e-02,\n          2.7550e+00,  2.9499e-01, -1.1396e+00, -1.4785e+00, -4.3375e+00,\n         -3.2104e-01, -3.2125e-01, -2.0806e+00,  3.7004e-01, -1.4368e+00,\n         -6.1700e-01, -2.0341e+00, -8.6155e-01, -4.0387e-01, -3.2359e-01,\n         -1.8287e+00, -1.7554e+00, -6.5640e-01,  6.7694e-01,  3.7156e+00,\n          2.1207e+00,  4.0970e+00,  1.7257e+00,  8.5265e-01,  1.2722e+00,\n          1.0563e+00,  1.3809e+00,  1.2871e+00, -7.5314e-01,  2.2593e+00,\n          1.1952e-01, -7.3866e-01,  1.0060e+00,  8.5880e-01, -6.6744e-01,\n         -3.2016e-01, -1.5605e+00,  2.0461e+00,  2.4740e+00,  2.2464e-01,\n          7.4987e-01,  3.8843e-02, -1.7622e+00,  1.9534e+00,  4.5175e-01,\n          1.2086e+00,  7.3219e-01, -1.0001e+00,  1.2820e-01, -3.7380e-01,\n          9.6213e-02,  3.2060e+00,  6.5023e-01, -1.1252e-01,  8.9641e-01,\n         -5.2856e-02, -1.1584e+00,  1.4922e-01,  3.7309e-01,  8.7084e-01,\n         -1.9354e+00,  1.0733e-01, -1.5175e+00, -1.8582e+00, -3.8437e+00,\n          1.8629e-01, -2.9438e+00,  5.4171e-01, -7.8057e-01, -2.6016e+00,\n         -4.4594e+00,  5.5604e-01, -1.3140e+00, -3.8408e+00, -7.5988e-01,\n         -5.7457e-01, -2.5448e+00,  2.3831e+00,  6.1368e-01,  4.8295e-01,\n          2.8674e+00, -3.7442e+00,  1.5085e+00, -3.2500e+00, -2.4894e+00,\n         -3.3541e-01,  1.2856e-01, -1.1355e+00,  3.3969e+00,  4.4584e+00]],\n       grad_fn=<AddmmBackward>)\n\n Size:  torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "out = resnet(batch_t)\n",
    "print(out)\n",
    "# TODO: Print the dimension of 'out'\n",
    "print(\"\\n Size: \", out.size())"
   ]
  },
  {
   "source": [
    "### 2.3 Interpreting the output\n",
    "\n",
    "You don't know what to do with that right? How do you know if this output tensor means that the image is a dog or a cat or something else? \n",
    "\n",
    "Well that's actually simple. The first idea would be to find the most activated output unit, that is to say, the index of max value and find the label with the corresponding index. To do so we use the [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html?highlight=max#torch.max) function\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index:  tensor([207]) \nLabel:  golden retriever \nOutput value:  tensor([15.6744], grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.max(out, 1)\n",
    "print(\n",
    "    \"Index: \",index,  \n",
    "    \"\\nLabel: \", labels[index], \n",
    "    \"\\nOutput value: \", out[0, index]\n",
    "    ) "
   ]
  },
  {
   "source": [
    "But now the question is how to interpret this output value? How can we say if the model hesitates between this label and another one? \n",
    "\n",
    "We would like to convert this tensor value into something that could be interpreted as the confidence that the model has in its prediction. To do so, we use the [softmax](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax) function which normalizes our outputs to \\[0, 1\\] and divide by the sum. \n",
    "\n",
    "For more information about the SoftMax function, watch the videos by Andrew Ng: \n",
    "- [Softmax Regression (C2W3L08)](https://www.youtube.com/watch?v=LLux1SW--oM)\n",
    "- [Training Softmax Classifier (C2W3L09)](https://www.youtube.com/watch?v=ueO_Ph0Pyqk)\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "1. Find the index corresponding to the max value of ``out`` **Hint:** Look at the previous cell "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label:  golden retriever \nConfidence:  96.29 %\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the index corresponding to the max value of out\n",
    "_, index = torch.max(out, 1)\n",
    "confidences = torch.nn.functional.softmax(out, dim=1)[0]\n",
    "percentages = confidences * 100\n",
    "print(\n",
    "    \"Label: \",labels[index[0]], \n",
    "    \"\\nConfidence: \", round(percentages[index[0]].item(), 2), \"%\")\n",
    "\n"
   ]
  },
  {
   "source": [
    "### 2.4 Top-1 and Top-5 errors\n",
    "\n",
    "When evaluating a image classifier we often use the terms *Top-1 error* and *Top-5 error* \n",
    "\n",
    "f the classifier’s top guess is the correct answer (e.g., the highest score is for the “dog” class, and the test image is actually of a dog), then the correct answer is said to be in the Top-1.\n",
    "\n",
    "If the correct answer is at least among the classifier’s top 5 guesses, it is said to be in the Top-5.\n",
    "\n",
    "The top-1 score is the conventional accuracy, that is to say its checks if the top class (the one having the highest confidence) is the same as the target label. This is what we have done in the cell above\n",
    "\n",
    "On the other hand, the top-5 score checks if the target label is one of your top 5 predictions (the 5 ones with the highest confidences). To do so we use the [torch.sort](https://pytorch.org/docs/stable/generated/torch.sort.html#torch-sort) function\n",
    "\n",
    "**QUESTIONS**\n",
    "\n",
    "1. Complete the code below **Hint:** Look at how we preprocessed the first image Bobby \n",
    "2. Does the model seem confident about the first prediction?\n",
    "\n",
    "**ANSWER**\n",
    "2. The model seems to be confident about the first prediction, as the confidence is 97.04%. Notable is, that all labels in the Top-5 are dogs, except for the third one, which is a tennis ball."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 :  \nLabel:  golden retriever \nConfidence:  97.04 %\n1 :  \nLabel:  cocker spaniel, English cocker spaniel, cocker \nConfidence:  0.48 %\n2 :  \nLabel:  tennis ball \nConfidence:  0.37 %\n3 :  \nLabel:  Pembroke, Pembroke Welsh corgi \nConfidence:  0.29 %\n4 :  \nLabel:  Irish setter, red setter \nConfidence:  0.2 %\n"
     ]
    }
   ],
   "source": [
    "num_preds = 5\n",
    "\n",
    "img = Image.open(\"imgs/golden_retriever_online.jpeg\")\n",
    "# TODO: preprocess the image\n",
    "img_t = preprocessor(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "# TODO: Compute the output tensor of the tensor image contained in img_t\n",
    "out = resnet(batch_t)\n",
    "# TODO: Compute the percentage representing the confidence of the model about the output\n",
    "percentages = (torch.nn.functional.softmax(out, dim=1)[0]) * 100\n",
    "_, indices = torch.sort(out, descending=True)\n",
    "\n",
    "results = [(labels[idx], round(percentages[idx].item(), 2)) for idx in indices[0][:num_preds]]\n",
    "for i_pred in range(num_preds):\n",
    "    print(\n",
    "        i_pred, \": \",\n",
    "        \"\\nLabel: \", results[i_pred][0], \n",
    "        \"\\nConfidence: \",  results[i_pred][1],\"%\"\n",
    "        )"
   ]
  },
  {
   "source": [
    "## 3. Playing with the ResNet model\n",
    "\n",
    "Put all the images that you want in the 'imgs/' (could be personal pictures or taken from the internet)\n",
    "\n",
    "**QUESTIONS**\n",
    "\n",
    "1. Complete the code below so that for each image it prints the 5 best guests according to the model\n",
    "2. When the image is a dog, what are usually the 1st, 2nd, 3rd guesses? \n",
    "3. Use one of your personal picture of an object whose label is in the list of labels.\n",
    "4. Try to find an image on the web whose label is in the list of labels but whose corresponding prediction is wrong. How can you try to make it difficult for the model to recognize the object? \n",
    "5. Try to find an image on the web whose label is NOT in the list of labels with which the model was trained. Look at the output, is it consistent even though it is necesseraly wrong? \n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "2. Images of dogs usually have different kinds of dog breeds for the first 3 results. Only golden_retriever_online.jpeg seems to be a tennis ball with a 0.37% confidence.\n",
    "3. coffee_and_cake.jpg is classified correctly. The model recognizes the coffe, plates cups and chocolate cake. The predictions for coffee_plant.jpeg are correct and the first choice has a high confidence as well.\n",
    "4. I used 'zebra_dog.jpg' which shows a dog wearing a zebra costume. The model classifies it as a zebra with 65% confidence and only has a dog as the third choice with 4.27% confidence. It gets difficult for the model to recognize an object, if it looks very similar to a different object that the model is also trained to recognize. \n",
    "5. I used the image 'carpet_internet.jpg'. The first prediction of the model is tray with a 79.75% confidence. The model mistakely classifies the carpet as a tray with a significant confidence. The other predictions in the Top-5 are closer: 'quilt, comforter, comfort, puff' with 8.11% confidence and 'prayer rug, prayer mat' with 1.31% confidence come at least close to the actual image."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " ======  Akita_personal.jpg  ====== \n",
      "0 :  \n",
      "Label:  Eskimo dog, husky \n",
      "Confidence:  20.56 %\n",
      "1 :  \n",
      "Label:  chow, chow chow \n",
      "Confidence:  20.46 %\n",
      "2 :  \n",
      "Label:  Samoyed, Samoyede \n",
      "Confidence:  20.06 %\n",
      "3 :  \n",
      "Label:  malamute, malemute, Alaskan malamute \n",
      "Confidence:  14.73 %\n",
      "4 :  \n",
      "Label:  golden retriever \n",
      "Confidence:  8.75 %\n",
      "\n",
      " ======  Bobby.jpeg  ====== \n",
      "0 :  \n",
      "Label:  golden retriever \n",
      "Confidence:  96.29 %\n",
      "1 :  \n",
      "Label:  Labrador retriever \n",
      "Confidence:  2.81 %\n",
      "2 :  \n",
      "Label:  cocker spaniel, English cocker spaniel, cocker \n",
      "Confidence:  0.28 %\n",
      "3 :  \n",
      "Label:  redbone \n",
      "Confidence:  0.21 %\n",
      "4 :  \n",
      "Label:  tennis ball \n",
      "Confidence:  0.12 %\n",
      "\n",
      " ======  cabin_by_the_sea.jpg  ====== \n",
      "0 :  \n",
      "Label:  breakwater, groin, groyne, mole, bulwark, seawall, jetty \n",
      "Confidence:  33.93 %\n",
      "1 :  \n",
      "Label:  promontory, headland, head, foreland \n",
      "Confidence:  26.32 %\n",
      "2 :  \n",
      "Label:  cliff, drop, drop-off \n",
      "Confidence:  13.83 %\n",
      "3 :  \n",
      "Label:  seashore, coast, seacoast, sea-coast \n",
      "Confidence:  10.51 %\n",
      "4 :  \n",
      "Label:  beacon, lighthouse, beacon light, pharos \n",
      "Confidence:  7.76 %\n",
      "\n",
      " ======  carpet_internet.jpg  ====== \n",
      "0 :  \n",
      "Label:  tray \n",
      "Confidence:  79.75 %\n",
      "1 :  \n",
      "Label:  quilt, comforter, comfort, puff \n",
      "Confidence:  8.11 %\n",
      "2 :  \n",
      "Label:  maze, labyrinth \n",
      "Confidence:  2.18 %\n",
      "3 :  \n",
      "Label:  dining table, board \n",
      "Confidence:  2.06 %\n",
      "4 :  \n",
      "Label:  prayer rug, prayer mat \n",
      "Confidence:  1.31 %\n",
      "\n",
      " ======  coffee_and_cake.jpg  ====== \n",
      "0 :  \n",
      "Label:  espresso \n",
      "Confidence:  75.48 %\n",
      "1 :  \n",
      "Label:  plate \n",
      "Confidence:  10.75 %\n",
      "2 :  \n",
      "Label:  cup \n",
      "Confidence:  2.2 %\n",
      "3 :  \n",
      "Label:  chocolate sauce, chocolate syrup \n",
      "Confidence:  1.7 %\n",
      "4 :  \n",
      "Label:  conch \n",
      "Confidence:  1.26 %\n",
      "\n",
      " ======  coffee_plant.jpeg  ====== \n",
      "0 :  \n",
      "Label:  pot, flowerpot \n",
      "Confidence:  99.23 %\n",
      "1 :  \n",
      "Label:  greenhouse, nursery, glasshouse \n",
      "Confidence:  0.15 %\n",
      "2 :  \n",
      "Label:  vase \n",
      "Confidence:  0.14 %\n",
      "3 :  \n",
      "Label:  doormat, welcome mat \n",
      "Confidence:  0.04 %\n",
      "4 :  \n",
      "Label:  window screen \n",
      "Confidence:  0.03 %\n",
      "\n",
      " ======  crochet_figures.jpg  ====== \n",
      "0 :  \n",
      "Label:  mitten \n",
      "Confidence:  42.59 %\n",
      "1 :  \n",
      "Label:  dishrag, dishcloth \n",
      "Confidence:  31.04 %\n",
      "2 :  \n",
      "Label:  teddy, teddy bear \n",
      "Confidence:  4.29 %\n",
      "3 :  \n",
      "Label:  wool, woolen, woollen \n",
      "Confidence:  3.0 %\n",
      "4 :  \n",
      "Label:  hair slide \n",
      "Confidence:  2.32 %\n",
      "\n",
      " ======  dog_meme.png  ====== \n",
      "0 :  \n",
      "Label:  dingo, warrigal, warragal, Canis dingo \n",
      "Confidence:  28.91 %\n",
      "1 :  \n",
      "Label:  chow, chow chow \n",
      "Confidence:  8.2 %\n",
      "2 :  \n",
      "Label:  Siberian husky \n",
      "Confidence:  7.86 %\n",
      "3 :  \n",
      "Label:  Eskimo dog, husky \n",
      "Confidence:  6.67 %\n",
      "4 :  \n",
      "Label:  Pembroke, Pembroke Welsh corgi \n",
      "Confidence:  5.65 %\n",
      "\n",
      " ======  golden_retriever_online.jpeg  ====== \n",
      "0 :  \n",
      "Label:  golden retriever \n",
      "Confidence:  97.04 %\n",
      "1 :  \n",
      "Label:  cocker spaniel, English cocker spaniel, cocker \n",
      "Confidence:  0.48 %\n",
      "2 :  \n",
      "Label:  tennis ball \n",
      "Confidence:  0.37 %\n",
      "3 :  \n",
      "Label:  Pembroke, Pembroke Welsh corgi \n",
      "Confidence:  0.29 %\n",
      "4 :  \n",
      "Label:  Irish setter, red setter \n",
      "Confidence:  0.2 %\n",
      "\n",
      " ======  husky_meme.png  ====== \n",
      "0 :  \n",
      "Label:  Siberian husky \n",
      "Confidence:  47.34 %\n",
      "1 :  \n",
      "Label:  Eskimo dog, husky \n",
      "Confidence:  42.7 %\n",
      "2 :  \n",
      "Label:  malamute, malemute, Alaskan malamute \n",
      "Confidence:  5.57 %\n",
      "3 :  \n",
      "Label:  Norwegian elkhound, elkhound \n",
      "Confidence:  3.0 %\n",
      "4 :  \n",
      "Label:  dogsled, dog sled, dog sleigh \n",
      "Confidence:  0.52 %\n",
      "\n",
      " ======  pier_private.jpg  ====== \n",
      "0 :  \n",
      "Label:  dock, dockage, docking facility \n",
      "Confidence:  22.5 %\n",
      "1 :  \n",
      "Label:  lakeside, lakeshore \n",
      "Confidence:  20.24 %\n",
      "2 :  \n",
      "Label:  speedboat \n",
      "Confidence:  18.56 %\n",
      "3 :  \n",
      "Label:  seashore, coast, seacoast, sea-coast \n",
      "Confidence:  17.26 %\n",
      "4 :  \n",
      "Label:  trimaran \n",
      "Confidence:  3.45 %\n",
      "\n",
      " ======  plushie_and_laptop.jpg  ====== \n",
      "0 :  \n",
      "Label:  teddy, teddy bear \n",
      "Confidence:  31.63 %\n",
      "1 :  \n",
      "Label:  vase \n",
      "Confidence:  10.26 %\n",
      "2 :  \n",
      "Label:  bakery, bakeshop, bakehouse \n",
      "Confidence:  8.48 %\n",
      "3 :  \n",
      "Label:  chocolate sauce, chocolate syrup \n",
      "Confidence:  6.78 %\n",
      "4 :  \n",
      "Label:  wool, woolen, woollen \n",
      "Confidence:  3.78 %\n",
      "\n",
      " ======  seal_meme01.png  ====== \n",
      "0 :  \n",
      "Label:  sea lion \n",
      "Confidence:  57.34 %\n",
      "1 :  \n",
      "Label:  otter \n",
      "Confidence:  22.36 %\n",
      "2 :  \n",
      "Label:  snow leopard, ounce, Panthera uncia \n",
      "Confidence:  5.65 %\n",
      "3 :  \n",
      "Label:  tabby, tabby cat \n",
      "Confidence:  4.99 %\n",
      "4 :  \n",
      "Label:  Egyptian cat \n",
      "Confidence:  3.32 %\n",
      "\n",
      " ======  seal_meme02.png  ====== \n",
      "0 :  \n",
      "Label:  sea lion \n",
      "Confidence:  76.4 %\n",
      "1 :  \n",
      "Label:  zebra \n",
      "Confidence:  4.83 %\n",
      "2 :  \n",
      "Label:  Arabian camel, dromedary, Camelus dromedarius \n",
      "Confidence:  3.2 %\n",
      "3 :  \n",
      "Label:  otter \n",
      "Confidence:  1.18 %\n",
      "4 :  \n",
      "Label:  meerkat, mierkat \n",
      "Confidence:  0.73 %\n",
      "\n",
      " ======  snowy_stairs.jpg  ====== \n",
      "0 :  \n",
      "Label:  maze, labyrinth \n",
      "Confidence:  18.71 %\n",
      "1 :  \n",
      "Label:  alp \n",
      "Confidence:  9.25 %\n",
      "2 :  \n",
      "Label:  shovel \n",
      "Confidence:  3.74 %\n",
      "3 :  \n",
      "Label:  stone wall \n",
      "Confidence:  3.08 %\n",
      "4 :  \n",
      "Label:  tank, army tank, armored combat vehicle, armoured combat vehicle \n",
      "Confidence:  2.88 %\n",
      "\n",
      " ======  zebra_dog.jpg  ====== \n",
      "0 :  \n",
      "Label:  zebra \n",
      "Confidence:  64.99 %\n",
      "1 :  \n",
      "Label:  gazelle \n",
      "Confidence:  12.5 %\n",
      "2 :  \n",
      "Label:  dalmatian, coach dog, carriage dog \n",
      "Confidence:  4.27 %\n",
      "3 :  \n",
      "Label:  EntleBucher \n",
      "Confidence:  0.82 %\n",
      "4 :  \n",
      "Label:  beagle \n",
      "Confidence:  0.75 %\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Load inputs\n",
    "# ------------------------------\n",
    "\n",
    "# Load all the images in the 'imgs/' folder\n",
    "list_img_t = []                  # Where input tensors will be stored\n",
    "path_imgs = 'imgs/'   \n",
    "list_files = listdir('imgs/')    # Find all filenames in the 'imgs/' folder\n",
    "for f in list_files:\n",
    "    img = Image.open(path_imgs + f)\n",
    "    img = img.convert('RGB')  # Because some of the images are in the RGBA format while ResNet requires a RGB format\n",
    "    img_t = preprocessor(img) # TODO: preprocess the image\n",
    "    list_img_t.append(torch.unsqueeze(img_t, 0) )\n",
    "\n",
    "# ------------------------------\n",
    "# Make predictions\n",
    "# ------------------------------\n",
    "num_preds = 5\n",
    "for i, batch_t in enumerate(list_img_t):\n",
    "    print(\"\\n ====== \", list_files[i], \" ====== \")\n",
    "\n",
    "    # TODO: Compute the output tensor of the tensor image contained in batch_t\n",
    "    out = resnet(batch_t)\n",
    "    # TODO: Compute the percentage representing the confidence of the model about the output\n",
    "    percentages = (torch.nn.functional.softmax(out, dim=1)[0]) * 100\n",
    "    # TODO: Sort the out tensor in descending order\n",
    "    _, indices = torch.sort(out, descending=True)\n",
    "    results = [(labels[idx], round(percentages[idx].item(), 2)) for idx in indices[0][:num_preds]]\n",
    "    for i_pred in range(num_preds):\n",
    "        print(\n",
    "            i_pred, \": \",\n",
    "            \"\\nLabel: \", results[i_pred][0], \n",
    "            \"\\nConfidence: \",  results[i_pred][1],\"%\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 4. Good to know\n",
    "- In PyTorch, data are stored in [tensors](https://pytorch.org/docs/stable/tensors.html#torch.Tensor). This is the counterpart of Numpy's array and most of the methods that are available with numpy arrays are also available with tensors. (e.g \n",
    "[size](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size), \n",
    "[amax](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.amax), \n",
    "[argmax](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.argmax), \n",
    "[sort](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sort), \n",
    "[abs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.abs), \n",
    "[cos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cos), \n",
    "[sum](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum) etc.)\n",
    "- In PyTorch all neural networks should be a class that is itself a subclass of the PyTorch's [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module) class\n",
    "- There are many well-known deep neural network architectures available in the [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html?highlight=models) sub-package. \n",
    "  - For each of these architecture a pre-trained model is available. \n",
    "  - Some of them such as the ResNet architecture even have multiple pre-trained model instances of different depths. Thus from the [ResNet](https://pytorch.org/docs/stable/torchvision/models.html#id10) class, we have [resnet18](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet18), [resnet50](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet50), [resnet101](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet101), etc.\n",
    "- During the preprocessing, we can use the [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms) module to perform the most common image transformations\n",
    "- Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, we use [model.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train) or [model.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval)\n",
    "- Top-1 and Top-5 scores are commenly used in image classification\n",
    "- When there are more than 2 possible classes we often use the [SoftMax]((https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax)) in the output layer to convert the output tensor vales into confidence values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}